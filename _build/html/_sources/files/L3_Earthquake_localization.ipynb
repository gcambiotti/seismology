{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fcad563-b1c4-4385-a82f-73a8af609565",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "source": [
    "(chap-L3)=\n",
    "# Earthquake localization\n",
    "\n",
    "```{contents} Sections\n",
    ":local:\n",
    ":depth: 2\n",
    "```\n",
    "\n",
    "```{div} full-width\n",
    "\n",
    "In this lesson, we will locate the origin of the 24/8/2016 M=6 Accumoli earthquake by exploiting the pickings of the first arrivals of the P and S waves obtained from the waveforms meeasured by a set of seismic station in the surrouning of the earthquake. In particular, our goal is to estimate the origin time, longitude, latitude and depth of the earthquake and to estimate the uncertainty of these estimates as well. \n",
    "\n",
    "In doing it, we will need a velocity model in order to modelling the several paths of the P and S waves from the hypocenter to the seismic stations and quantify the travel times. In particular, we will adopt the same velocity model used by the Istituto Nazionale di Geofisica e Vulcanologia for locating most of the earthquakes occurred in Italy. It is composed by the upper and lower crusts and the lithospheric mantle. The Conrad (between the upper and lower crusts) and Moho (between the lower crust and the lithospheric mantle) are set at 11 and 26.9 km depth.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c8649-5c21-4ebd-89fd-d4e68ff179ca",
   "metadata": {},
   "source": [
    "## Inverse problem theory\n",
    "\n",
    "```{div} full-width\n",
    "\n",
    "We now rewiev the minimal aspects ot the inverse problem thoery that we need for estimating the origin time and spatial coordinates of an earthquake. More details about this topic can be found in {numref}`Appendix %s - Inverse problem theory <chap-I>`, including the difference between volumetric and density probability functions and how taking into account modelling errors.  \n",
    "\n",
    "```\n",
    "\n",
    "### Observation equation\n",
    "\n",
    "```{div} full-width\n",
    "\n",
    "Let us write the so called observation equation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& y = f(m) + \\varepsilon \n",
    "\\end{align}\n",
    "$$ (E:1)\n",
    "\n",
    "where $f$ is the function expressing the theoretical relationship between the model parameters $m$ and the observations $y$, and $\\varepsilon$ are the random errors that we assume to obey to a (N-dimensional) Gaussian distribution with zero mean and covariance $\\mathrm{C}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& p(\\varepsilon) = \\frac{1}{\\sqrt{(2\\,\\pi)^N\\,\\det(C)}}\\,\\exp\\left(-\\frac{\\varepsilon^T\\,C^{-1}\\,\\varepsilon}{2}\\right) \n",
    "\\end{align}\n",
    "$$ (E:2)\n",
    "\n",
    "In particular, $y$, $\\varepsilon$ and $m$ are the arrays collecting the $N$ observations $y_i$, the $N$ random errors $\\varepsilon_i$ and the $M$ model parameters $m_i$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& y = \\big(y_1,\\cdots,y_N\\big) \\qquad \\varepsilon = \\big(\\varepsilon_1,\\cdots,\\varepsilon_N\\big) \\qquad m =(m_1,\\cdots,m_M)\n",
    "\\end{align}\n",
    "$$ (E:3)\n",
    "\n",
    "Just because $\\varepsilon=y-f(m)$, we can see eq. {eq}`E:2` as the conditional probability denstity function (PDF) for the observations given the model parameters \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& p(y|m) = \\frac{1}{\\sqrt{(2\\,\\pi)^N\\,\\det(C)}}\\,\\exp\\left(-\\frac{W(m)}{2}\\right) \n",
    "\\end{align}\n",
    "$$ (E:4)\n",
    "\n",
    "with $W$ being the so called weighted residual square sum (WRSS)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& W(m) = (y-f(m))^T\\,C^{-1}\\,(y-f(m))\n",
    "\\end{align}\n",
    "$$ (E:5)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "<font size=\"4\"> Homogeneous half-space model </font>\n",
    "\n",
    "```{div} full-width\n",
    "\n",
    "Let us now consider the simple half-space homogeneous Earth models and denote with $c_p$ and $c_s$ being the velocities of the P and S waves, respectively, and the model parameters $m$ as the origin time $t_e$ and spatial coordinates $\\mathbf{x}_e$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& m=(t_e,\\mathbf{x}_e)\n",
    "\\end{align}\n",
    "$$ (E:6)\n",
    "\n",
    "In this case, the observation equation {eq}`E:1` can be further specified as follows\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& y_i = f_i(m) + \\varepsilon_i  \\qquad\n",
    "\\end{align}\n",
    "$$ (E:7)\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& f_i(m) =  t_e + \\frac{|\\mathbf{x}_i-\\mathbf{x}_e|}{c_i}\n",
    "\\end{align}\n",
    "$$ (E:8)\n",
    "\n",
    "Here, $y_i$ and $\\varepsilon_i$ are the time and the random error of the $i$-th pickings, while $c_i$ and $\\mathbf{x}_j$ are  the seismic velocity of its phase ($c_p$ or $c_s$) and the spatial coordinates of its seismic station. Furthermore, assuming that there are no modelling errors and that the observations are independent, the covariance $C$ of the random errors is a simple diagonal matrix\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& C = \\mathrm{diag}(\\sigma_1^2,\\cdots,\\sigma_N^2)\n",
    "\\end{align}\n",
    "$$ (E:9)\n",
    "\n",
    "with the variances $\\sigma_i^2$ as diagonal elements. Here, $\\sigma_i$ is the standard deviation of the $i$-th picking. \n",
    "\n",
    "We note that, in this simple case, the inverse of the covariance $C $ simply reads\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& C ^{-1} = \\mathrm{diag}(\\sigma_1^{-2},\\cdots,\\sigma_N^{-2})\n",
    "\\end{align}\n",
    "$$ (E:10)\n",
    "\n",
    "and the WRSS, eq. {eq}`E:5` simplifies into\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& W(m) = \\sum_{i=1}^N \\left(\\frac{y_i-f_i(m)}{\\sigma_i}\\right)^2\n",
    "\\end{align}\n",
    "$$ (E:11)\n",
    "\n",
    "Recasted in this form, we can understand that each term of the summation defining the WRSS is the residual square $(y_i-f_i(m))^2$ weighted by the variance $\\sigma_i^2$.\n",
    "\n",
    "```\n",
    "\n",
    "### Posterior probability density function (PDF)\n",
    "\n",
    "```{div} full-width\n",
    "\n",
    "The posterior PDF is the conditional PDF $p(m|y)$ of the model parameters given the observaions. In order to obtain the posterior PDF, let us now assume that the prior information on the model parameters are described by the PDF $p(m)$ and make use of it for defining the PDF for both the observations and the model parameters\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& p(y,m) = p(y|m)\\,p(m)\n",
    "\\end{align}\n",
    "$$ (E:12)\n",
    "\n",
    "In this respect the prior information can be seen as the marginal PDF for the model parameters \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& p(m) =\\int_{\\mathbb{R}^N} p(y,m)\\,d y = p(m)\\,\\int_{\\mathbb{R}^N} p(y|m)\\,d y  = p(m)\n",
    "\\end{align}\n",
    "$$ (E:13)\n",
    "\n",
    "In order to obtain the posterior PDF $p(m|y)$, we rewrite eq. {eq}`E:12` with the roles of $y$ and $m$ reversed\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& p(y,m) = p(m|y)\\,p(y)\n",
    "\\end{align}\n",
    "$$ (E:14)\n",
    "\n",
    "with $p(y)$ being the marginal PDF for the observation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& p(y) = \\int_{\\mathcal{M}} p(y,m)\\,dm\n",
    "\\end{align}\n",
    "$$ (E:15)\n",
    "\n",
    "where $\\mathcal{M}\\subseteq\\mathbb{R}^M$ is the model parameter space. The latter can be a subset of the M-dimensional space just because some model parameters can be constrained to some sepcific region of it, as for instance the vertical (or radial) coordinate of the origin that we assume to be non-positive (i.e., we will consider only positive depth, within the solid Earth).\n",
    "\n",
    "From the comparison between eqs. {eq}`E:12` and {eq}`E:14`, we thus obtain the so called Bayes' theorem\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& p(m|y) = \\frac{p(y|m)\\,p(m)}{p(y)}\n",
    "\\end{align}\n",
    "$$ (E:16)\n",
    "\n",
    "We note that the denominator $p(y)$ of the right-hand side of eq. {eq}`E:16` is needed only for normalizing the posterior PDF and, so, we can write\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& p(m|y) \\propto p(y|m)\\,p(m)\n",
    "\\end{align}\n",
    "$$ (E:17)\n",
    "\n",
    "Further assuming that we do not have prior information and that the prior PDF is constant, the Bayes' theorem simply states that the posterior PDF is simply proportional to the conditional PDF of the observations given the model parameters\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& p(m|y) \\propto p(y|m)\n",
    "\\end{align}\n",
    "$$ (E:18)\n",
    "\n",
    "```\n",
    "\n",
    "### Additional weights\n",
    "\n",
    "```{div} full-width\n",
    "\n",
    "As we will see, the pickings from the Istituto Nazionale di Geofisica e Vulcanologia are provided with both the uncertainties (i.e., their standard derivations $\\sigma_i$) and additional weights $w_i$. This means that the WRSS, eq. {eq}`E:11` is actualy modified as follows\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "& \\displaystyle\n",
    "W(m) = \\sum_{i=1}^N w_i\\,\\left(\\frac{y_i-f_i(m)}{\\sigma_i}\\right)^2 \n",
    "\\end{align}\n",
    "$$ (E:19)\n",
    "\n",
    "where each weighted residual square is multiplied by the additional weight $w_i$. The use of additional weight is quite common in the earthquake localization because it allows to give more weights to the early pickings (i.e., to the pickings of those seismic stations closer to the earthquake origin). It easy to understand that their introduction is equivalent to scale the original standard deviations $\\sigma_i$ by $w_i^{-1/2}$\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "& \\displaystyle\n",
    "W(m) = \\sum_{i=1}^N \\left(\\frac{y_i-f_i(m)}{\\sigma_i\\,w_i^{-1/2}}\\right)^2 \n",
    "\\end{align}\n",
    "$$ (E:20)\n",
    "\n",
    "In light of this and of eqs. {eq}`E:4` and {eq}`E:18`, we thus obtain\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& p(m|y) = k\\,\\exp\\left(-\\frac{W(m)}{2}\\right)\n",
    "\\end{align}\n",
    "$$ (E:21)\n",
    "\n",
    "where $k$ is a normalization constant that we will not need to determine. \n",
    "\n",
    "Neglecting the differences between volumetric and density probability functions discussed in {numref}`Appendix %s - Inverse problem theory <chap-I>`, we can estimate the best (or most likelihood) model parameters by minimizing thw WRSS. In other words, the model parameteres $m^*$ that minimize $W$ will be our estimate of the origin time and spatial coordinates of the earthquake\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    " W(m^*) = \\min_{m\\in\\mathcal{M}} W(m) &\n",
    "\\end{align}\n",
    "$$ (E:22)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fad021-0aa3-40a0-8714-98224bfff47f",
   "metadata": {},
   "source": [
    "## Data set\n",
    "\n",
    "```{div} full-width\n",
    "\n",
    "In this section we will obtain all the information that we need to define the inverse problem, which means that we will obtain the picking times and all the additional information that we need for implementing the theoretical relationship between the model parameters and the observations, as well as calculating the WRSS.\n",
    "\n",
    "```\n",
    "\n",
    "### Earthquake selection\n",
    "\n",
    "```{div} full-width\n",
    "\n",
    "Let us start by importing the main python libraries\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f28c46-dc03-45aa-b2ba-a7cc302a765c",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import lab\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c40cd35-85bf-4a64-a3b9-6ef1a09f826b",
   "metadata": {
    "tags": [
     "full-width",
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from myst_nb import glue\n",
    "np.set_printoptions(precision=3,suppress=True)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7372e1aa-8d44-4a57-aa59-081f2a7a449a",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "and reading the seismic inventory and the catalog containing only the 2016-08-24 M6 Accumuli earthquake that we downloaded in the  {numref}`Lecture %s <chap-L1>` {ref}`chap-L1`\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1369f27d-a5bc-474e-837d-ee7df26b041c",
   "metadata": {
    "tags": [
     "full-width",
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "from obspy.core.event import read_events\n",
    "from obspy.core.inventory import read_inventory\n",
    "\n",
    "directory = \"Amatrice-Norcia-Visso/\"\n",
    "\n",
    "file = directory+\"Seismic_Inventory.xml\"\n",
    "inventory = read_inventory(file)\n",
    "\n",
    "file = directory + \"7073641_Accumuli.xml\"\n",
    "catalog = read_events(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ac3831-aee0-4dac-9eab-6242c2ed0c04",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "and we select the only event in the catalog \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa1c9d-1e06-41d4-bc2c-4a012f1843db",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = catalog[0]\n",
    "print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43df1f-e975-4481-8e4d-de79ab7f84ba",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "Giving a look to the picks, we can understand that each `pick` contains information like the waveform from which has been evalueted, the phase of seismic (P or S) waves considered, the time (in `UTCDateTime`) of its arrival at the seismic station and the author who performed the measurement\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f95b6-40b0-4ce1-b0a6-6aa35737df0f",
   "metadata": {
    "tags": [
     "output_scroll",
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "for pick in event.picks:\n",
    "    print(pick)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b2000-af72-4f09-bc5a-3800cc5aa1c5",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "Also, checking the preferred `origin` of the event, we can see that now there is an additional field named `arrivals`\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113b7e0-9628-498f-a7ba-c6b7f8a35da9",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "origin = event.preferred_origin()\n",
    "print(origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721c4058-9b18-484d-bb07-02e2ed867ea1",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "Giving a look to the list `origin.arrivals`, we can understand that each `arrival` contains information like the `pick_id` (which can be used for associated the specific `arrival` to one `pick` of the list of the `event.picks`), the epicentral `distance` (in degrees) from the estimated origin to the seismic station, the `time_residual` of the observed and modelled travel time ànd the `time_weight` (in percent, so we shall divide them by 100) that we already discussed in section ()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec3d8b9-bd8b-4a97-80b9-e42fbb1b59a6",
   "metadata": {
    "tags": [
     "full-width",
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "for arrival in origin.arrivals:\n",
    "    print(arrival)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c650702b-6ade-4881-ae23-e4604d20d452",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "Checking the number of picks and of arrivals, we can see that only a fraction of the former (about one third) have been actually used for locating the earthquake\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30b35a-ee20-43a9-a893-25fd0e16fe09",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"number of picks:    \",len(event.picks))\n",
    "print(\"number of arrivals: \",len(origin.arrivals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788c258a-5b7c-46b0-b8e8-67b1ac23d4e0",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "This is because the list `event.picks` contains all the picking made by all the authors that studied this earthquakes, while the list `origin.arrivals` contains only the arrivals used for estimating the preferred `origin`.  Also, as we can see, each `arrival` have a field `pick_id` that can be used to associate it to a specific `pick`. We will exploit this fact in the next section.\n",
    " \n",
    "\n",
    "In addition to it, plotting the time weights of the arrivals against the epicentral distance, we can see that the assigned weight decrease with the epicentral distance and that only arrivals within the first 100 km have been actually used (the weights at greater distances being zeros). Also, we can see that all the arrivals for the S waves are weighted by about one half with respect to the arrivals for P waves at similar distances. This is beacuse the picking of the S waves is much more questionable than that of the P waves.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c806ef5-8f56-474a-8088-bfdee8f9f6fb",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "earth_radius = 6371.009\n",
    "deg2km = np.pi/180 * earth_radius\n",
    "\n",
    "fig,ax = plt.subplots(tight_layout=True)\n",
    "for phase in [\"P\",\"S\"]:\n",
    "    distances = [ arrival.distance * deg2km for arrival in origin.arrivals if arrival.phase == phase]\n",
    "    weights   = [ arrival.time_weight / 100 for arrival in origin.arrivals if arrival.phase == phase]\n",
    "    ax.scatter(distances,weights,label=phase,marker=\".\")\n",
    "ax.set_xlabel(\"Distance [km]\")\n",
    "ax.set_ylabel(\"Weight\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62627e76-21f8-4ac3-8277-985f25066e19",
   "metadata": {},
   "source": [
    "### Picking selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1f25ad-a567-4513-94d2-49c7a6a2dafa",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "Let us thus select only those picks that have been used in the earthquake localization of the preferred origin\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e984b-6ef7-439a-8fbb-9d3a8b9f7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_ids = [arrival.pick_id for arrival in origin.arrivals]\n",
    "origin_picks = [pick for pick in event.picks if pick.resource_id in pick_ids]\n",
    "\n",
    "print(\"Number of picks:                            \",len(event.picks))\n",
    "print(\"Number of arrivals:                         \",len(origin.arrivals))\n",
    "print(\"Number of picks associated to the arrivals: \",len(origin_picks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b290f86a-0a45-4ba5-9425-c30482e2a2fc",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "As we can see, we succeed in selecting only the picks associated to the arrivals.\n",
    "\n",
    "Let us now obtain and organize all the informations that we need for defining the inverse problem of the earthquake location with similar settings to those used by the Istituto Nazionale di Geofisica e vulcanologia. First we make a lists of picks and arrivals where, for each arrival with a non-zero weight we associated the corresponding pick.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a8755-66f1-4941-93a2-84f037352c6e",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "picks_arrivals = []\n",
    "for arrival in origin.arrivals:\n",
    "    if arrival.time_weight == 0.0: continue\n",
    "    for pick in origin_picks:\n",
    "        if pick.resource_id == arrival.pick_id:\n",
    "            picks_arrivals.append([pick,arrival])\n",
    "            break\n",
    "print(\"Number of selected picks and arrivals:\",len(picks_arrivals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6601b8f-37ef-4173-993d-ebdc8e17ff71",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "From this list (which contains only 56 couples of picks and arrivals, because all the others have zero weight) we can obtain the piciking time and its uncertainity (or standard deviations) from the pick and the assigned weight from the arrival. In addition to it, we shall obtain the spatial coordinate of the seismic station (longitude, latitude and elevation) to which each picking refers. Unfortunatly, the geopgarphical coordinate are not ready available niether from the pick or the arrival and, so, we shall make use of the waveform identifiers contained in the picks along with an inventory of the seismic networks from which we can obtain the geographical coordinates. In this perspective, we download all the seismic stations within about 100 km (1 degrees) from the epicenter that have operate one hour before and one hour after the origin time.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510b846-5aae-45b5-baa1-507bba862725",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "circle = dict(maxradius=1,longitude=origin.longitude,latitude=origin.latitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cb9f6c-f358-4adb-bef6-42851ed69485",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "and plot it along with the catalog containing the only Accumoli earthquake\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66664b-1586-4064-934a-b7403195cf5c",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "fig = lab.setup_map(circle,color=\"red\")\n",
    "fig = inventory.plot(fig=fig,label=False,size=30,show=False,color=\"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53e2940-8773-4c9f-a9fb-68448db8f0f3",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "Then, we extent the list of picks and arrivals including also the channels from the inventory corresponding to those to which the picks refer\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dfdfa1-1b8e-4f31-878a-1f8275935faa",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "picks_arrivals_channels = []\n",
    "for pick, arrival in picks_arrivals:\n",
    "    waveform_id = pick.waveform_id.id\n",
    "    codes = waveform_id.split(\".\")\n",
    "    inv = inventory.select(*codes)\n",
    "    try:\n",
    "        channel = inv[0][0][0]\n",
    "        picks_arrivals_channels.append([pick,arrival,channel])\n",
    "    except:\n",
    "        print(\"Warning! There is no the channel \"+waveform_id+\" to which the pick refer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00607a94-c372-4ecf-b920-3bba9a959519",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "The `try` and `except` syntax has been used to prevent from an error in the case in which the channel is not found in the inventory. Fortunately, in the present case, all the channels that we need have been found.\n",
    "\n",
    "In order to have an idea of how many seismic stations have been considered by the INGV for locating the earthquake, we now make the same plot as before but indicating with the green mark (and the station code) those seismic stations that have been actually used.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3cb451-5263-456b-b449-eeda3a9a9c5e",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "import cartopy\n",
    "\n",
    "fig = lab.setup_map(circle,color=\"red\")\n",
    "fig = inventory.plot(fig=fig,label=False,size=30,show=False,color=\"orange\")\n",
    "\n",
    "ax = fig.axes[0]\n",
    "for pick, arrival, channel in picks_arrivals_channels:\n",
    "    label = pick.waveform_id.id.split(\".\")[1]\n",
    "    ax.scatter(channel.longitude,channel.latitude,marker=\"v\",color=\"green\",transform=cartopy.crs.PlateCarree(),zorder=30)\n",
    "    ax.annotate(label,xy=(channel.longitude+0.02,channel.latitude),transform=cartopy.crs.PlateCarree(),zorder=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4107a1e6-6494-43f7-a242-6dbccb6a73b4",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "Among all these channels, it will be useful to identify the closest one, i.e., the one for which has been made the early picking\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6c405-2861-42c3-bdc8-729afa614588",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [ pick.time for pick, _, _ in picks_arrivals_channels ]\n",
    "\n",
    "i0 = np.argmin(times)\n",
    "ref_pick, ref_arrival, ref_channel = picks_arrivals_channels[i0]\n",
    "\n",
    "ref_time = ref_pick.time\n",
    "ref_longitude, ref_latitude = ref_channel.longitude, ref_channel.latitude\n",
    "ref_code = ref_pick.waveform_id.id\n",
    "\n",
    "print(\"ref_code: \",ref_code)\n",
    "print(\"ref_time:\",ref_time)\n",
    "print(ref_longitude,ref_latitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41e2983-bdf0-4051-9e95-4247d4792ba7",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from myst_nb import glue\n",
    "glue(\"ref_code\", ref_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f56efa-8256-499b-a748-d05cc9a77082",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "As far as we know, without performing the full inversion of the observations, the picking time, longitude and latitude of the channels {glue:}`ref_code` are the closest one to the actual origin and, so, we can use it as first guess for minimizing the WRSS.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4177b-4ae3-4f27-825e-857ecc4d45f7",
   "metadata": {},
   "source": [
    "### Data organization and the azimuthal equidistant projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a9911-afd5-475d-ae21-8af90c24bf07",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "We are now in the position of obtaining all the informations that we need for defining the inverse problem: the picking time, the longitude, latitude and elevation of the seismic station, the picking uncertainity, the weight assigned to the arrival and the phase. \n",
    "\n",
    "Rather than obtaining the picking time in UTCDateTime, we will consider a reference time and compute the differences with respect to it in order to obtain the picking time in seconds\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f3ac4d-5007-441a-99e0-79c8bddcbda3",
   "metadata": {
    "tags": [
     "full-width",
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "N = len(picks_arrivals_channels)\n",
    "data = np.empty((N,6))\n",
    "for i,(pick,arrival,channel) in enumerate(picks_arrivals_channels):\n",
    "        \n",
    "        T = pick.time - ref_time\n",
    "        E = pick.time_errors[\"uncertainty\"]\n",
    "        scale = arrival.time_weight/100\n",
    "        E /= np.sqrt(scale)\n",
    "\n",
    "        lon,lat = channel.longitude, channel.latitude\n",
    "        Z = channel.elevation\n",
    "        \n",
    "        phase = int(pick.phase_hint == \"S\")\n",
    "\n",
    "        data[i] = [T,lon,lat,Z,E,phase]\n",
    "        \n",
    "print(\"{0:>9}  {1:>9}  {2:>9}  {3:>9}  {4:>9}  {5:>9}\".format(\"t [s]\",\"lat [°]\",\"lon [°]\",\"z [m]\",\"std [s]\",\"phase\"))\n",
    "for datum in data:\n",
    "    print(\"{0:>9.4f}  {1:>9.4f}  {2:>9.4f}  {3:>9.4f}  {4:>9.4f}  {5:>9d}\".format(*datum[:-1],int(datum[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c38bb6-7c77-47bd-992c-92ccec1772f7",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "In order to simplify the modelling of the travel times of the seismic wave to the seismic station and because we are considering a very small area of the Earth surface, we will consider a Cartesian reference system and use the elevation as vertical coordinate $x_3$ and the $x_1$ and $x_2$ coordinates of the azimuthal equidistant projection for the horizontal coordinates. The azimuthal equidistant projection is implemented in the module `cartopy`. As already discussed in {numref}`Lecture %s <chap-L1>` {ref}`chap-geographic-maps`, here the syntax required for the coordinate change from the geographical reference system to the Cartesian one\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3530ba-683d-4a02-903d-6962ce60dc5e",
   "metadata": {
    "tags": [
     "full-width",
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "projection = cartopy.crs.AzimuthalEquidistant(central_longitude=ref_longitude, central_latitude=ref_latitude)\n",
    "geodetic   = cartopy.crs.Geodetic()\n",
    "\n",
    "llz = data[:,1:4].T  #longitude, latitude, elevation\n",
    "xyz = projection.transform_points(geodetic,*llz)\n",
    "xyz /= 1e3\n",
    "\n",
    "data[:,1:4] = xyz #Cartesian coordinates x, y, z\n",
    "\n",
    "print(\"{0:>9}  {1:>9}  {2:>9}  {3:>9}  {4:>9}  {5:>9}\".format(\"t [s]\",\"x [km]\",\"y [km]\",\"z [km]\",\"std [s]\",\"phase\"))\n",
    "for datum in data:\n",
    "    print(\"{0:>9.4f}  {1:>9.4f}  {2:>9.4f}  {3:>9.4f}  {4:>9.4f}  {5:>9d}\".format(*datum[:-1],int(datum[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de968468-9628-4653-8a95-6b2c224bc3ff",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "ingv = np.array([origin.time-ref_time,origin.longitude,origin.latitude,-origin.depth]) # [s,deg,deg,m]\n",
    "llz = ingv[1:]  #longitude, latitude, elevation\n",
    "xyz = projection.transform_points(geodetic,*llz)\n",
    "xyz /= 1e3\n",
    "ingv[1:] = xyz #Cartesian coordinates x, y, z\n",
    "\n",
    "\n",
    "deg2km = np.pi/180 * earth_radius\n",
    "\n",
    "ingv_std = np.empty(4)\n",
    "ingv_std[0] = origin.time_errors[\"uncertainty\"]\n",
    "rlat = origin.latitude * np.pi/180\n",
    "ingv_std[1] = origin.longitude_errors[\"uncertainty\"] * deg2km * np.cos(rlat)\n",
    "ingv_std[2] = origin.latitude_errors[\"uncertainty\"]  * deg2km\n",
    "ingv_std[3] = origin.depth_errors[\"uncertainty\"] / 1e3\n",
    "\n",
    "keys = [\"T0 [s]\",\"X0 [km]\",\"Y0 [km]\",\"Z0 [km]\"]\n",
    "\n",
    "print(\"\\n{0:>8} {1:>8} {2:>8} \".format(\"\",\"val\",\"std\"))\n",
    "for key,val,std in zip(keys,ingv,ingv_std):\n",
    "    print(\"{0:<8} {1:>8.4f} {2:>8.4f}\".format(key,val,std))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81dfed7-894a-4d1c-99e3-41907308aa5c",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "In the list of arrivals it is also contained the time residual of each arrival, that is the difference between the picking time and the one modelled with the velocity model of the INGV. We use it to obtain the modelled times by INGV and we plot the data with respect to the origin and compare them with respect to the modelled ones\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d47cd5-e33a-4832-bd3c-be3ddbb50566",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "residues = np.array([arrival.time_residual for _, arrival, _ in picks_arrivals_channels])\n",
    "modelled_times = data[0]- residues \n",
    "fig = lab.plot_data(data,origin=ingv,modelled_times=modelled_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c52caaf-2353-42d3-9463-3b9f51a4333c",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "Let us now make the same figure plotting also two straight lines according to a simple velocity model with $v_p = 5$ km/s and $v_s = 3$ km/s\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c4cb6-0855-4aef-8183-de3498478729",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "fig = lab.plot_data(data,origin=ingv,modelled_times=modelled_times)\n",
    "T0,T1 = ingv[0],15\n",
    "fig.axes[0].plot([T0,T1],[0,5*(T1-T0)],color=\"gold\");\n",
    "fig.axes[0].plot([T0,T1],[0,3*(T1-T0)],color=\"gold\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a309d-87d3-4093-b7bd-cba9d45b6bb7",
   "metadata": {},
   "source": [
    "## Homogeneous half-space velocity model\n",
    "\n",
    "```{div} full-width\n",
    "\n",
    "In the simple case of an homogeneous half-space velocity model, we only need to specify the P and S wave seismic velocity\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b197c35-adfe-41a8-a5bc-00a082200646",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "velocity_model = np.array([6,3])\n",
    "print(\"velocity model:\",velocity_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505c3bbd-19dc-4056-a0e6-c626c5ac9cd8",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "Let us define a function which compute the travel times for a set of hypocentral distances `Ds`, taking info account for the seismic phases `Ps` and the specified velocity model. This function correspons to the thoeoretical relationship between the model parameters and the observations, eq. {eq}`E:8`.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152ed79-2dcf-4d99-beb5-25b0c3534d78",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "def eva_travel_times(Ds,Ps,velocity_model):\n",
    "    \n",
    "    VP,VS = velocity_model\n",
    "        \n",
    "    DTs_mod = np.empty(len(Ds))\n",
    "    for i,(D,P) in enumerate(zip(Ds,Ps)):\n",
    "        if P == 1:\n",
    "            DTs_mod[i] = D/VS\n",
    "        else:\n",
    "            DTs_mod[i] = D/VP\n",
    "\n",
    "    return DTs_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d8e73e-f0e7-407c-8c7e-a60b7aa31f16",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "Then, we use it for defining another function which compute the weighte residual square sum (WRSS)), eq. {eq}`E:19`, for the model parameters $m$ collecting the origin time `TE` and cartesian coordinates `XE`, `YE` and `ZE` once given the `data` and the `velocity_model`\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b06416-3cdb-4642-bdd7-4d47b0090da1",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "def eva_wrss(m,data,velocity_model):\n",
    "    \n",
    "    TE,XE,YE,ZE = m\n",
    "    Ts,Xs,Ys,Zs,Es,Ps = data\n",
    "    Ds = np.sqrt( (Xs-XE)**2 + (Ys-YE)**2 + (Zs-ZE)**2 )\n",
    "\n",
    "    Ts_mod = eva_travel_times(Ds,Ps,velocity_model) + TE\n",
    "\n",
    "    WRSS = (((Ts-Ts_mod)/Es)**2).sum()\n",
    "        \n",
    "    return WRSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d010938-afd9-4d62-afd4-3237e3cc756b",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "Here the so calculated WRSS for the origin `ingv` by the INGV according to the homogeneous half-space velocity model\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba20a33-6e6f-415b-b141-acb4f99a4a6c",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "wrss = eva_wrss(ingv,data,velocity_model)\n",
    "print(\"wrss for the ingv:\",wrss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c625d7b0-dba5-4c0f-931e-bee643c3daca",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "Making use of the submodule `optimize` from `scipy`, we can define the model space using the class `optimize.Bounds` and minimize the WRSS as function of the model parameters using the function `optimize.minimize`. By default, in presence of `bounds`, the minimization is made according to the L-BFGS-B method described in [Byrd et al. (1995)](https://epubs.siam.org/doi/10.1137/0916069) and [Zhu et al. (1997)](https://dl.acm.org/doi/10.1145/279232.279236) (click {download}`here<../_static/Byrd_etal_1995.pdf>` and {download}`here<../_static/Zhu_Byrd_1997.pdf>` for downloading the pdfs) .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e5486-a2de-4807-8d7d-3a1d5d2dd9b0",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "ub = [np.inf,np.inf,np.inf,0]\n",
    "bounds = optimize.Bounds(ub=ub)  # definition of the upper bounds for the model parameters (i.e., the vertical coordinate ZE must be non-positive\n",
    "\n",
    "fun = lambda m: eva_wrss(m,data,velocity_model)   # function to be minimized \n",
    "\n",
    "guess = [0,0,0,-10]   # starting point for the minimization\n",
    "sol = optimize.minimize(fun, guess, bounds=bounds)     # minimizationb\n",
    "print(\"Minimization solution:\\n\\n\",sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866425a5-fbc2-4022-ae97-3c92fcb9d759",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "\n",
    "We note that we have made use of the `lambda` syntax for defining the function `fun` of the only model parameters starting from the python function `eva_wrss`. Also, the function `optimize.minimize` return the object  `sol` which summarize the main information about the mninimization, including where the function takes its minimum, at `sol.x`, and the minimum value `sol.fun`.\n",
    "\n",
    "In order to verify that we found a minimum, let us plot the WRSS by varying one model parameter at the time and fixing the other to their most likelihood values\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3072f06f-4806-4c81-be38-37c6a052b1f1",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "def plot_wrss(mb, exts, fun):\n",
    "\n",
    "    fig,axes = plt.subplots(2,2,tight_layout=True,figsize=(8,8))\n",
    "    axes = axes.flatten()\n",
    "    for k,(key,ext) in enumerate(exts.items()):\n",
    "        ax = axes[k]\n",
    "        m = mb.copy()\n",
    "        xs = np.linspace(*ext,1000) + mb[k]\n",
    "        fs = np.empty(len(xs))\n",
    "        for i,x in enumerate(xs):\n",
    "            m[k] = x\n",
    "            WRSS = fun(m)\n",
    "            fs[i] = WRSS\n",
    "        ax.plot(xs,fs)\n",
    "        ax.axvline(mb[k],linewidth=0.5)\n",
    "        ax.axhline(fs.min(),linewidth=0.5)\n",
    "        ax.set_xlabel(key)\n",
    "        ax.set_ylabel(\"WRSS\")\n",
    "        \n",
    "    wrss = fun(mb)\n",
    "    text = \"WRSS: {0:>10.3f}    Model parameters: \"+\" \".join([ \"{\"+str(k)+\":>7.3f}\" for k in range(1,5) ])\n",
    "    fig.suptitle(text.format(wrss,*mb))\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa73a3-d4d1-46d3-bd24-105124f0af0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exts = { \"TE [s]\":[-0.2,0.2], \"XE [km]\": [-1,1], \"YE [km]\": [-1,1], \"ZE [km]\":[-1,1] }\n",
    "fig = plot_wrss(sol.x, exts, fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c516a-8c48-408b-9efa-59f43e05ae65",
   "metadata": {},
   "source": [
    "## Two layered half-space velocity model\n",
    "\n",
    "```{div} full-width\n",
    "\n",
    "Let us imporve the velocity model by considering a two-layered Earth model consisting of the upper and lower crusts separated by the Conrad discontinuity at $11$ km depth. \n",
    "\n",
    "In doing this, let us consider a Cartesian reference system and denote with $\\ell$ and $z$ the horizontal and vertical cordinates of the seismic station and set the hypocenter at $(0,z_e)$, with $z_e\\leq 0$ and $z\\geq 0$, as depicted in {numref}`Figure {number} <fig-velocity-model>`. \n",
    "\n",
    "```\n",
    "\n",
    "````{div} full-width\n",
    "```{figure} ../images/Velocity_model.png\n",
    "---\n",
    "align: center\n",
    "name: fig-velocity-model\n",
    "---\n",
    "Cartoon depicting the Cartesian reference system and the ray path from the hypocenter at $(0,z_e)$ to the seismic station at $(\\ell,z)$. The dashed line indicates the Conrad discontinuity at $z=H=-11$ km. We assume that the vertical coordinate of the seismic station is non-negative, $z\\geq 0$, and denote with $\\ell_c$ the horizontal component of the ray path within the lower crust.\n",
    "```\n",
    "````\n",
    "\n",
    "<font size=\"4\"> Hypocenter in the upper crust </font>\n",
    "\n",
    "```{div} full-width\n",
    "\n",
    "When the hypocenter is in the upper crust ($z_e \\geq -H$), we have to compare the direct (red line) and headwave (green line) travel times $t_d$ and $t_h$ and select the smaller one\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& t = \\min(t_d,t_h)\n",
    "\\end{align}\n",
    "$$ (E:23)\n",
    "\n",
    "with \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& t_d = \\frac{\\sqrt{(z-z_e)^2 + \\ell^2}}{c_0}\n",
    "\\qquad\\mathrm{and}\\qquad\n",
    "t_h = \\frac{\\ell_c}{c_1} + \\frac{ \\sqrt{\\Delta z^2 + \\Delta \\ell^2}}{c_1}\n",
    "\\end{align}\n",
    "$$ (E:24)\n",
    "\n",
    "Here, $c_0$ and $c_1$ are the seismic wave velocities of the upper and lower crusts, while $\\Delta \\ell$ and $\\Delta z$ are the horizontal and vertical components of the ray path of the headwave in the upper crust\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\Delta l = \\ell-\\ell_c \\geq 0 \\qquad\\mathrm{and}\\qquad\n",
    "\\Delta z = (z_e-H) + (z-H)  = z_e + z - 2\\,H\n",
    "\\end{align}\n",
    "$$ (E:25)\n",
    "\n",
    "In order to determine $\\ell_c$, we note that the incident (or reflection) angle $\\theta$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\tan\\theta = \\frac{\\Delta \\ell}{\\Delta z}\n",
    "\\end{align}\n",
    "$$ (E:26)\n",
    "\n",
    "and that the refraction angle (in the lower crust) must be $\\pi/2$. In this respect, from the Snell's law, the indident angle must be\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&  \\sin\\theta = \\frac{c_0}{c_1}  \\qquad\\Rightarrow\\qquad \\tan\\theta = \\frac{\\sin\\theta}{\\cos\\theta} = \\frac{c_0}{\\sqrt{c_1^2-c_0^2}}\n",
    "\\end{align}\n",
    "$$ (E:27)\n",
    "\n",
    "We thus obtain \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\Delta \\ell = \\tan\\theta\\,\\Delta z\\qquad\\Rightarrow\\qquad\n",
    "\\ell_c = \\ell - \\Delta\\ell = \\ell - \\tan\\theta\\,\\Delta z\n",
    "\\end{align}\n",
    "$$ (E:28)\n",
    "\n",
    "and we can rewrite the headwave travel time as follows\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& t_h = \\frac{\\ell_c}{v_1} + \\Delta \\ell\\,\\sqrt{\\frac{c_1^2}{c_0^2}-1}\n",
    "\\end{align}\n",
    "$$ (E:29)\n",
    "\n",
    "We note that the headwave is possible only when $\\Delta \\ell \\geq 0$ or, equivalently, $\\ell\\geq \\ell_c$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc8feb6-6326-4c89-b52e-7361b4763ee2",
   "metadata": {},
   "source": [
    "\n",
    "<font size=\"4\"> Hypocenter in the lower crust </font>\n",
    "\n",
    "```{div} full-width\n",
    "\n",
    "\n",
    "When the hypocenter depth is the lower crust ($z_e<-H$), the travel time $t$ depend on the horizontal coordinate $x$ where the ray path encounter the Conrad discontinuity\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& t_d(\\ell_c) = \\frac{ \\sqrt{\\big(H-z_e\\big)^2+\\ell_c^2}}{v_1} + \\frac{ \\sqrt{(z-H)^2+\\big(\\ell-\\ell_c\\big)^2} }{v_0}\n",
    "\\end{align}\n",
    "$$ (E:30)\n",
    "\n",
    "\n",
    "\n",
    "In order to determine $\\ell_c$ we have to minimize eq. {eq}`E:30` within the interval  $[0,L]$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\frac{d t_d(\\ell_c)}{d\\ell_c} = \\frac{\\ell_c}{v_1\\,\\sqrt{(H-z_e)^2+\\ell_c^2}} - \\frac{\\ell-\\ell_c}{v_0\\,\\sqrt{(z-H)^2+(\\ell-\\ell_c)^2}} = 0\n",
    "\\end{align}\n",
    "$$ (E:31)\n",
    "\n",
    "By equating the square of the two terms in the right-hand side and multuplying by the minimum common denominator, eq. {eq}`E:31` becomes\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\ell_c^2\\,(\\ell-\\ell_c)^2\\,(v_1^2-v_0^2) - \\ell_c^2\\,v_0^2\\,(z-H)^2 + (\\ell-\\ell_c)^2\\,v_1^2\\,(H-z_e)^2 = 0\n",
    "\\end{align}\n",
    "$$ (E:32)\n",
    "\n",
    "Then, by introducing the following normalization\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\ell_c = L\\,\\hat{\\ell_c} \n",
    "\\end{align}\n",
    "$$ (E:33)\n",
    "\n",
    "and defining\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& A_0 = \\frac{v_0^2}{v_1^2-v_0^2}\\,\\frac{(z-H)^2}{\\ell^2} = \\frac{S\\,B_0}{1-S} & \\quad  A_1 = \\frac{v_1^2}{v_1^2-v_0^2}\\,\\frac{(H-z_e)^2}{\\ell^2} = \\frac{B_1}{1-S}\n",
    "\\end{align}\n",
    "$$  (E:34)\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& S = \\frac{v_0^2}{v_1^2} & \\quad B_0 = \\frac{(z-H)^2}{\\ell^2}  \\qquad B_1 = \\frac{(H-z_e)^2}{\\ell^2}\n",
    "\\end{align}\n",
    "$$  (E:35)\n",
    "\n",
    "we obtain that $\\hat{\\ell}_c\\in[0,1]$ must be a root of the following 4-order polynomial\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\hat{\\ell}^4\\,-2\\,\\hat{\\ell}^3  + \\hat{\\ell}^2\\,(1 + A_1-A_0) - 2\\,\\hat{\\ell}\\,A_1 + A_1 = 0\n",
    "\\end{align}\n",
    "$$  (E:36)\n",
    "\n",
    "By definition, there is only one real root of the above polynomial in the normalized interval $[0,1]$.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f2b61a-3ba5-4ccb-88ae-3073ff75dd4f",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "```{div} full-width\n",
    "\n",
    "Here, we define the two layered velocity model\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe80715-35e1-483d-9c8b-f05fa415636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_model = [5.00, 6.50, 2.89, 3.75, -11.1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6344bc83-5500-493b-9671-5f1446757e14",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "and we implement the python function computing the modelled travel time of the two-layered Earth model \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c4600-aa7d-40ea-a52c-8d17bb72ca4f",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "def eva_travel_time(L, Z, ZE, V0, V1, H):\n",
    "    \n",
    "    sin0 = V0/V1\n",
    "    S = sin0**2\n",
    "    \n",
    "    if ZE >= H:\n",
    "        \n",
    "        D = np.sqrt(L**2+(Z-ZE)**2)\n",
    "        T = D/V0\n",
    "        \n",
    "        if sin0 < 1:\n",
    "        \n",
    "            cos0 = np.sqrt(1-S)\n",
    "\n",
    "            DZ  = Z + ZE - 2*H \n",
    "\n",
    "            D0 = DZ/cos0\n",
    "            DX = D0*sin0\n",
    "\n",
    "            if DX < L:\n",
    "\n",
    "                T_headwave = (L-DX)/V1  +  D0/V0\n",
    "                T = min(T,T_headwave)\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        if V0 == V1:\n",
    "            \n",
    "            D = np.sqrt(L**2+(Z-ZE)**2)\n",
    "            T = D/V0\n",
    "\n",
    "        elif L == 0:\n",
    "\n",
    "            T = (Z-H)/V0 + (H-ZE)/V1\n",
    "            \n",
    "        else:\n",
    "\n",
    "            B0 = ( (Z-H)/L )**2\n",
    "            B1 = ( (H-ZE)/L )**2\n",
    "            A0 = B0 * S / (1-S)\n",
    "            A1 = B1 / (1-S)\n",
    "\n",
    "            poly_coefs = [1,-2,1+A1-A0,-2*A1,A1]\n",
    "            \n",
    "            roots = np.roots(poly_coefs)\n",
    "            real_roots = roots[roots.imag == 0].real\n",
    "            \n",
    "            X = real_roots[np.logical_and(real_roots >= 0,real_roots<=1)][0]\n",
    "            \n",
    "            T = L * ( np.sqrt(B1+X**2)/V1 + np.sqrt(B0+(1-X)**2)/V0 )\n",
    "        \n",
    "    return T\n",
    "#################################################################\n",
    "\n",
    "\n",
    "#################################################################\n",
    "def eva_travel_times(Ls,Zs,Ps,ZE,velocity_model):\n",
    "    \n",
    "    VP0,VP1,VS0,VS1,H = velocity_model\n",
    "\n",
    "    DTs_mod = np.empty(len(Ls))        \n",
    "    for i,(L,Z,P) in enumerate(zip(Ls,Zs,Ps)):\n",
    "        if P == 1:\n",
    "            DTs_mod[i] = eva_travel_time(L,Z,ZE,VS0,VS1,H)\n",
    "        else:\n",
    "            DTs_mod[i] = eva_travel_time(L,Z,ZE,VP0,VP1,H)\n",
    "        \n",
    "    return DTs_mod\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4fcce5-11e6-479a-9c5f-168096cbbcbe",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "Before solving the inverse problem of the earthquake localization, let us plot the travel times as function of the hypocentral distance for the cases $z_e=-7$ and $-13$ km (above and below the Conrad discontinuity)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae7f9d0-3216-4bce-b427-ce8723dd8404",
   "metadata": {
    "tags": [
     "full-width",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,3,tight_layout=True,figsize=(10,5))\n",
    "\n",
    "for ZE in [-7,-13]:\n",
    "\n",
    "    label = str(-ZE)+\" [km]\"\n",
    "    \n",
    "    Ls = np.linspace(0,100,1000)\n",
    "    Zs = np.zeros(Ls.shape)\n",
    "    Ps = np.zeros(Ls.shape)\n",
    "\n",
    "    Ds1 = np.sqrt(Ls**2+(Zs-ZE)**2)\n",
    "    Ts1 = eva_travel_times(Ls,Zs,Ps,ZE,velocity_model)\n",
    "\n",
    "    Ls = 10**np.linspace(0,4,1000)\n",
    "    \n",
    "    Ds2 = np.sqrt(Ls**2+(Zs-ZE)**2)\n",
    "    Ts2 = eva_travel_times(Ls,Zs,Ps,ZE,velocity_model)\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.plot(Ts1,Ds1,label=label)\n",
    "\n",
    "    axes[1].plot(Ds1/Ts1,Ds1,label=label)\n",
    "    axes[2].semilogy(Ds2/Ts2,Ds2,label=label)\n",
    "\n",
    "axes[0].set_ylabel(\"Hypocentral distance [km]\")\n",
    "axes[0].set_xlabel(\"Travel time [s]\")\n",
    "axes[1].set_xlabel(\"Distance / Time [km/s]\")        \n",
    "axes[2].set_xlabel(\"Distance / Time [km/s]\")        \n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(0,linewidth=0.5)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8cfaef-00cb-4a38-8e89-a706faa21548",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "As we can see ...\n",
    "\n",
    "Let us now make ude of the function `eva_travel_times` for compouting the WRSS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d81d4-b7c9-481b-a1b5-2a50e715e17f",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "def eva_wrss(m,data,velocity_model,logy=False):\n",
    "    \n",
    "    TE,XE,YE,ZE = m\n",
    "    Ts,Xs,Ys,Zs,Es,Ps = data\n",
    "    Ls = np.sqrt( (Xs-XE)**2 + (Ys-YE)**2 )\n",
    "\n",
    "    Ts_mod = eva_travel_times(Ls,Zs,Ps,ZE,velocity_model) + TE\n",
    "\n",
    "    WRSS = (((Ts-Ts_mod)/Es)**2).sum()\n",
    "        \n",
    "    if logy:\n",
    "        return WRSS, Ts_mod\n",
    "    else:\n",
    "        return WRSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6905b3ab-194b-467c-8cf9-3331dec17321",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "wrss = eva_wrss(ingv,data,velocity_model)\n",
    "print(\"wrss:\",wrss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6665aaeb-17fa-486a-a481-f1216e19840f",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "wrss, Ts_mod = eva_wrss(ingv,data,velocity_model,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f3b77-dc39-449c-8675-7c4905607eac",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "fig = lab.plot_data(data,ingv,Ts_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd2315f-d78c-447e-8983-1996e1af08ae",
   "metadata": {},
   "source": [
    "```{div} full-width\n",
    "Let us now consider the starting point that we already use for the homogeneous half-space model and find the minimum of the WRSS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08c35a-62e0-493b-9d91-bb7fec5b3326",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "fun = lambda m: eva_wrss(m,data,velocity_model)\n",
    "sol = optimize.minimize(fun,guess,bounds=bounds)\n",
    "\n",
    "print(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c39e22-1045-4cb5-b25c-5014bfe79a39",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "exts = { \"TE\":[-0.2,0.2], \"XE\":[-1,1], \"YE\":[-1,1], \"ZE\":[-1,1] }\n",
    "fig = plot_wrss(sol.x, exts, fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e9cec-59b8-4570-b351-85b2b8174d45",
   "metadata": {},
   "source": [
    "### Searching for multiple local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b33fb7-044e-46e8-a96c-f6872490a130",
   "metadata": {
    "tags": [
     "full-width",
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "def make_guesses(nwalkers,lims):\n",
    "\n",
    "    spans = np.diff(lims,axis=1)\n",
    "    print(spans[:,0])\n",
    "    print(lims[:,0])\n",
    "    ndim = len(lims)\n",
    "    guesses = np.random.rand(nwalkers, ndim) * spans[:,0] + lims[:,0]\n",
    "\n",
    "    return guesses\n",
    "\n",
    "nwalkers = 50\n",
    "mlims = np.array([[-5,0],[-20,20],[-20,20],[-20,0]])\n",
    "\n",
    "guesses = make_guesses(nwalkers,mlims)\n",
    "print(\"\\nmean =\",guesses.mean(axis=0))\n",
    "print(\"std  =\",guesses.std(axis=0))\n",
    "print(guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91aa9c3-09a5-4b4d-a686-65a5e0299fa0",
   "metadata": {
    "tags": [
     "full-width",
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "sols = []\n",
    "for i,guess in enumerate(guesses):\n",
    "    sol = optimize.minimize(fun,guess,bounds=bounds)\n",
    "    print(i,guess,sol.x,sol.fun)\n",
    "    sols.append(sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b63ac8-7c89-4ad0-b691-72d7059663b8",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "delta_max = 1e-3\n",
    "\n",
    "uniques = []\n",
    "for i,sol_new in enumerate(sols):\n",
    "    logy = True\n",
    "    for j in uniques:\n",
    "        sol = sols[j]\n",
    "        delta = np.sqrt(( (sol.x-sol_new.x)**2 ).sum())\n",
    "        if delta < delta_max:\n",
    "            logy = False\n",
    "    if logy: \n",
    "        uniques.append(i)\n",
    "        print(\"best =\",sol.x,\"fun =\",sol.fun)\n",
    "        \n",
    "uniques = np.array(uniques)\n",
    "print(\"uniques =\",uniques)\n",
    "\n",
    "js = np.argsort([sols[j].fun for j in uniques])\n",
    "uniques = uniques[js]\n",
    "print(\"sorted uniques =\",uniques)\n",
    "\n",
    "sols = [sols[j] for j in uniques]\n",
    "for sol in sols:\n",
    "    fig = plot_wrss(sol.x, exts, fun)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ce79e-1889-4bd0-84b5-0781a55b9b1f",
   "metadata": {},
   "source": [
    "### Estimates of the uncercainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c698d2-93ce-4d03-ae89-41901730528c",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "###########################################################s\n",
    "def eva_functional(fun,m,N):\n",
    "    \n",
    "    wrss = fun(m)\n",
    "    L = N * np.log(wrss)\n",
    "    \n",
    "    return L\n",
    "###########################################################s\n",
    "\n",
    "from scipy import linalg\n",
    "\n",
    "###########################################################s\n",
    "def make_inverse_symmetric(hess):\n",
    "\n",
    "    w,o = linalg.eigh(hess)\n",
    "    inv = (o / w) @ o.T\n",
    "    return inv\n",
    "###########################################################s\n",
    "\n",
    "\n",
    "###########################################################s\n",
    "def make_hess(functional,mb,steps):\n",
    "\n",
    "    M = len(mb)\n",
    "    H = np.empty((M,M))\n",
    "    f0 = functional(mb)\n",
    "    \n",
    "    dm = np.eye(M) * steps\n",
    "    for i in range(M):\n",
    "        eps = steps[i]\n",
    "        f_up = functional(mb+dm[i]) \n",
    "        f_dw = functional(mb-dm[i])\n",
    "        H[i,i] = ( f_up + f_dw - 2*f0 ) / steps[i]**2\n",
    "        for j in range(i):\n",
    "            up =  ( functional(mb+dm[i]/2+dm[j]/2) - functional(mb-dm[i]/2+dm[j]/2) ) / steps[i]\n",
    "            dw =  ( functional(mb+dm[i]/2-dm[j]/2) - functional(mb-dm[i]/2-dm[j]/2) ) / steps[i]\n",
    "            H[i,j] = (up-dw) / steps[j]\n",
    "            H[j,i] = H[i,j]\n",
    "        \n",
    "    return H\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3922eb-d894-4200-a5ff-b4583fdba470",
   "metadata": {
    "tags": [
     "full-width"
    ]
   },
   "outputs": [],
   "source": [
    "functional = lambda m: eva_functional(fun,m,N)\n",
    "\n",
    "steps = np.ones(M)*1e-4\n",
    "\n",
    "ms = np.empty((len(sols),M))\n",
    "Ws = np.empty(len(sols))\n",
    "Ls = np.empty(len(sols))\n",
    "covs = np.empty((len(sols),M,M))\n",
    "for i,sol in enumerate(sols):\n",
    "    m = sol.x\n",
    "    ms[i] = sol.x\n",
    "    Ws[i] = sol.fun\n",
    "    Ls[i] = functional(m)\n",
    "    H = make_hess(functional,m,steps)\n",
    "    covs[i] = 2*make_inverse_symmetric(H)\n",
    "    print(Ws[i])\n",
    "    print(ms[i])\n",
    "    print(np.sqrt(np.diag(covs[i])))\n",
    "DL = Ls[0]-Ls[1]\n",
    "print(DL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8008c9-f6aa-47c2-b59b-0d7647fb9d6d",
   "metadata": {},
   "source": [
    "<p style=\"page-break-after:always;\"></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
